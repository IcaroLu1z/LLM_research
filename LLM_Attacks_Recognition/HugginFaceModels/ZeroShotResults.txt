nohup: ignoring input
cuda:0

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.46s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.05s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.26s/it]
The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.
Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.
Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`

Classification Report:
              precision    recall  f1-score   support

      Attack       0.70      0.67      0.68     20000
      Normal       0.68      0.71      0.70     20000

    accuracy                           0.69     40000
   macro avg       0.69      0.69      0.69     40000
weighted avg       0.69      0.69      0.69     40000

Execution time: 20:59:35.55 (h:mm:ss)
